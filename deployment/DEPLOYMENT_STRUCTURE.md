# Deployment Structure

This deployment creates a complete Bacalhau compute node with sensor simulation capabilities.

## Components Deployed

### 1. Bacalhau Compute Node
- **Service**: Docker container running Bacalhau
- **Config**: `/etc/bacalhau/config.yaml`
- **Orchestrator**: Connects to configured orchestrator endpoint
- **Data Access**: Can read sensor data from `/opt/sensor/data`
- **AWS Access**: Has access to AWS credentials if provided

### 2. Sensor Log Generator
- **Service**: Docker container running sensor simulator
- **Port**: 8080 (web interface)
- **Data Output**: `/opt/sensor/data/`
- **Config**: `/opt/sensor/config/sensor-config.yaml`
- **Identity**: Unique node identity generated per instance

### 3. Node Identity
- **Generated By**: `generate_node_identity.py`
- **Location**: `/opt/sensor/config/node-identity.json`
- **Based On**: EC2 instance ID (deterministic)
- **Contains**: City, GPS coordinates, sensor metadata

## Directory Structure

```
/opt/
├── bacalhau/
│   ├── docker-compose.yml
│   └── config.yaml
├── sensor/
│   ├── docker-compose.yml
│   ├── generate_node_identity.py
│   ├── sensor-config.yaml
│   ├── config/
│   │   ├── node-identity.json (generated)
│   │   └── sensor-config.yaml
│   └── data/ (sensor output)
└── deployment/ (extracted tarball)

/etc/
├── bacalhau/
│   └── config.yaml
└── systemd/system/
    └── bacalhau.service

/root/.aws/
└── credentials (if provided)
```

## Services Running

After deployment, you'll have:

1. **bacalhau-compute-1** - Bacalhau compute node
2. **sensor-log-generator** - Sensor data simulator

## Checking Status

SSH into an instance and run:

```bash
# Check all services
docker ps

# Check Bacalhau logs
docker compose -f /opt/bacalhau/docker-compose.yml logs -f

# Check sensor logs
docker compose -f /opt/sensor/docker-compose.yml logs -f

# View sensor data being generated
ls -la /opt/sensor/data/

# Check node identity
cat /opt/sensor/config/node-identity.json | jq .
```

## AWS Credentials

To enable S3 access:

1. Create `deployment/etc/aws/credentials` file (use `credentials.example` as template)
2. Add your AWS access keys
3. Redeploy instances

The credentials will be available to Bacalhau jobs for pushing data to S3.

## Sensor Data Access

Bacalhau jobs can access sensor data using local input source:

```yaml
InputSources:
  - Source:
      Type: localDirectory
      Params:
        SourcePath: /opt/sensor/data
        ReadWrite: false
    Target: /inputs/sensor-data
```

## Monitoring

- Sensor web interface: `http://<instance-ip>:8080`
- Bacalhau API: `https://<instance-ip>:1234` (with TLS)

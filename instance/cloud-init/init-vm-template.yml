#cloud-config

write_files:
  - path: /tmp/deployment.tar.gz
    encoding: base64
    content: ${compressed_deployment}
    permissions: "0600"
  - path: /usr/local/bin/deploy_instance_files.py
    permissions: "0755"
    content: |
      #!/usr/bin/env python3
      """Deploy instance files from tarball to their proper locations."""
      import os
      import shutil
      import subprocess
      import sys
      import tarfile
      from pathlib import Path

      def main():
          tarball_path = "/tmp/deployment.tar.gz"
          extract_dir = "/tmp/instance-deployment"
          
          if not os.path.exists(tarball_path):
              print(f"ERROR: Tarball not found at {tarball_path}")
              sys.exit(1)
          
          try:
              print(f"Extracting {tarball_path} to {extract_dir}")
              os.makedirs(extract_dir, exist_ok=True)
              
              with tarfile.open(tarball_path, "r:gz") as tar:
                  tar.extractall(extract_dir)
              
              instance_files_dir = Path(extract_dir) / "instance-files"
              
              if not instance_files_dir.exists():
                  print(f"ERROR: instance-files directory not found in tarball")
                  sys.exit(1)
              
              print("Deploying files to their target locations...")
              deploy_directory_structure(instance_files_dir, Path("/"))
              
              print("Cleaning up temporary files...")
              shutil.rmtree(extract_dir, ignore_errors=True)
              os.remove(tarball_path)
              
              print("âœ… Deployment complete!")
              
          except Exception as e:
              print(f"ERROR during deployment: {e}")
              sys.exit(1)

      def deploy_directory_structure(source_dir: Path, target_root: Path):
          for item in source_dir.rglob("*"):
              if item.is_file():
                  relative_path = item.relative_to(source_dir)
                  target_path = target_root / relative_path
                  
                  target_path.parent.mkdir(parents=True, exist_ok=True)
                  
                  print(f"  Deploying: {relative_path} -> {target_path}")
                  shutil.copy2(item, target_path)
                  
                  if target_path.suffix == ".py" or target_path.suffix == ".sh":
                      os.chmod(target_path, 0o755)
                  
                  try:
                      subprocess.run(["chown", "ubuntu:ubuntu", str(target_path)], 
                                   check=False, capture_output=True)
                  except Exception:
                      pass

      if __name__ == "__main__":
          main()

users:
  - name: ${username}
    sudo: ALL=(ALL) NOPASSWD:ALL
    shell: /bin/bash
    ssh_authorized_keys:
      - ${public_ssh_key}
    groups: docker

package_update: true
package_upgrade: true

runcmd:
  # Update packages and install basics
  - apt update
  - apt install -y $all_packages python3 curl wget

  # Install Docker
  - curl -fsSL https://get.docker.com -o get-docker.sh
  - sh get-docker.sh
  - usermod -aG docker ${username}

  # Set up SSH keys properly  
  - mkdir -p /home/${username}/.ssh
  - echo "${public_ssh_key}" > /home/${username}/.ssh/authorized_keys
  - chown -R ${username}:${username} /home/${username}/.ssh
  - chmod 700 /home/${username}/.ssh
  - chmod 600 /home/${username}/.ssh/authorized_keys

  # Deploy instance files from tarball structure
  - python3 /usr/local/bin/deploy_instance_files.py

  # Create necessary directories 
  - mkdir -p ${bacalhau_data_dir} ${bacalhau_node_dir} /opt/bacalhau_node

  # Enable and start services
  - systemctl daemon-reload
  - systemctl enable bacalhau.service
  - systemctl enable sensor.service
  - chmod 0400 ${bacalhau_node_dir}/docker-compose.yaml
  - chmod 0777 ${bacalhau_data_dir}

  # Set ownership
  - chown -R ${username}:${username} ${bacalhau_data_dir}
  - chown -R ${username}:${username} ${bacalhau_node_dir}
  - chown ${username}:${username} ${bacalhau_node_dir}/config.yaml
  - chown ${username}:${username} ${bacalhau_node_dir}/docker-compose.yaml

  # Add user to docker group
  - usermod -aG docker ${username}

  # Install uv globally and set permissions
  - export HOME=/root
  - curl -LsSf https://astral.sh/uv/install.sh | env UV_INSTALL_DIR="/usr/local/bin" HOME=/root sh
  - chmod 755 /usr/local/bin/uv || true
  - chown ${username}:${username} /usr/local/bin/uv || true

  # Create uv cache directory for user
  - mkdir -p /home/${username}/.cache/uv
  - chown -R ${username}:${username} /home/${username}/.cache

  # Configure Docker to start on boot
  - systemctl enable docker.service
  - systemctl enable containerd.service

  # Start services
  - systemctl daemon-reload
  - systemctl enable docker
  - systemctl start docker
  - systemctl enable bacalhau.service
  - systemctl start bacalhau.service

  - uv run /usr/local/bin/startup.py

  # Run additional commands if file exists
  - test -f /tmp/exs/additional_commands.sh && sh -c /tmp/exs/additional_commands.sh

power_state:
  mode: reboot
  condition: True

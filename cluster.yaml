name: databricks-wind-farm-cluster

# Global deployment - one node per zone across the world
num_nodes: 9

resources:
  # Multi-cloud worldwide deployment - SkyPilot will spread across zones globally
  infra: aws
  # This automatically distributes nodes across available zones worldwide:
  # - us-west-2a (US West - Oregon)
  # - us-east-1a (US East - Virginia)
  # - eu-west-1a (Europe - Ireland)
  # - ap-south-1a (Asia Pacific - Mumbai)
  # - ap-southeast-2a (Asia Pacific - Sydney)
  # - sa-east-1a (South America - SÃ£o Paulo)
  # - And more zones as needed for fault tolerance

  # Instance configuration optimized for spot
  instance_type: t3.medium
  use_spot: true
  disk_size: 30

  # Ensure public IPs for external communication
  # Note: SkyPilot automatically handles security groups and ingress

# File mounts - deploy all necessary files
file_mounts:
  # Credentials (these files must exist locally)
  /tmp/credentials/orchestrator_endpoint: ./credentials/orchestrator_endpoint
  /tmp/credentials/orchestrator_token: ./credentials/orchestrator_token
  /tmp/credentials/aws-credentials: ./credentials/aws-credentials

  # Configuration files
  /tmp/config/bacalhau-config-template.yaml: ./instance/config/bacalhau-config-template.yaml
  /tmp/config/sensor-config.yaml: ./config/sensor-config.yaml

  # Docker Compose files
  /tmp/compose/bacalhau-compose.yml: ./compose/bacalhau-compose.yml
  /tmp/compose/sensor-compose.yml: ./compose/sensor-compose.yml

  # Scripts
  /tmp/scripts/generate_node_identity.py: ./scripts/generate_node_identity.py
  /tmp/scripts/health_check.sh: ./scripts/health_check.sh
  /tmp/scripts/generate_bacalhau_config.py: ./scripts/generate_bacalhau_config.py

# Environment variables
envs:
  # Deployment tagging
  DEPLOYMENT_PROJECT: "databricks-wind-farm-cluster"
  DEPLOYMENT_ID: "${CLUSTER_DEPLOYMENT_ID}"
  DEPLOYMENT_PURPOSE: "bacalhau-compute-node"

  # Bacalhau configuration
  BACALHAU_DATA_DIR: "/bacalhau_data"
  BACALHAU_NODE_DIR: "/bacalhau_node"
  BACALHAU_CONFIG_DIR: "/etc/bacalhau"
  BACALHAU_DISABLEANALYTICS: "true"
  LOG_LEVEL: "info"

  # Sensor configuration
  SENSOR_DATA_DIR: "/opt/sensor/data"
  SENSOR_CONFIG_DIR: "/opt/sensor/config"
  SENSOR_EXPORTS_DIR: "/opt/sensor/exports"

  # Network configuration
  BACALHAU_API_PORT: "1234"
  BACALHAU_NATS_PORT: "4222"

# Setup phase - install dependencies and prepare environment
setup: |
  set -e

  # Update system
  sudo apt-get update -qq

  # Install essential packages
  sudo apt-get install -y \
    python3 \
    python3-pip \
    curl \
    wget \
    unzip \
    jq \
    net-tools

  # Install Docker
  curl -fsSL https://get.docker.com | sudo sh
  sudo usermod -aG docker $USER

  # Install Docker Compose plugin
  sudo apt-get install -y docker-compose-plugin

  # Create directory structure
  sudo mkdir -p \
    /etc/bacalhau \
    /bacalhau_data \
    /bacalhau_node \
    /opt/bacalhau \
    /opt/sensor/config \
    /opt/sensor/data \
    /opt/sensor/exports \
    /opt/credentials \
    /opt/config \
    /opt/compose \
    /opt/scripts \
    /var/log/bacalhau \
    /root/.aws \
    /home/ubuntu/.aws

  # Set proper ownership
  sudo chown -R ubuntu:ubuntu \
    /opt/bacalhau \
    /opt/sensor \
    /bacalhau_data \
    /bacalhau_node \
    /home/ubuntu/.aws

  # Move uploaded files to proper locations
  sudo mv /tmp/credentials/* /opt/credentials/ 2>/dev/null || echo "No credentials files to move"
  sudo mv /tmp/compose/* /opt/compose/ 2>/dev/null || echo "No compose files to move"
  sudo mv /tmp/scripts/* /opt/scripts/ 2>/dev/null || echo "No script files to move"

  # Move config files to correct locations
  sudo mv /tmp/config/bacalhau-config-template.yaml /opt/config/ 2>/dev/null || echo "No bacalhau config to move"
  sudo mv /tmp/config/sensor-config.yaml /opt/sensor/config/ 2>/dev/null || echo "No sensor config to move"

  # Make scripts executable
  sudo chmod +x /opt/scripts/*.sh /opt/scripts/*.py

  # Set up AWS credentials for containers
  sudo cp /opt/credentials/aws-credentials /root/.aws/credentials 2>/dev/null || true
  sudo cp /opt/credentials/aws-credentials /home/ubuntu/.aws/credentials 2>/dev/null || true
  sudo chmod 600 /root/.aws/credentials /home/ubuntu/.aws/credentials 2>/dev/null || true

  # Tag AWS resources for proper identification and cleanup
  INSTANCE_ID=$(curl -s http://169.254.169.254/latest/meta-data/instance-id 2>/dev/null || echo "unknown")
  REGION=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone 2>/dev/null | sed 's/.$//' || echo "us-west-2")

  if [ "$INSTANCE_ID" != "unknown" ] && [ "$REGION" != "unknown" ]; then
    # Install AWS CLI if not present
    if ! command -v aws &> /dev/null; then
      curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
      unzip awscliv2.zip
      sudo ./aws/install
      rm -rf aws awscliv2.zip
    fi

    # Tag the instance
    aws ec2 create-tags --region "$REGION" --resources "$INSTANCE_ID" --tags \
      Key=Project,Value=databricks-wind-farm-cluster \
      Key=DeploymentID,Value="${DEPLOYMENT_ID}" \
      Key=Purpose,Value=bacalhau-compute-node \
      Key=ManagedBy,Value=skypilot \
      Key=Environment,Value=production 2>/dev/null || true

    # Tag associated volumes
    VOLUMES=$(aws ec2 describe-instances --region "$REGION" --instance-ids "$INSTANCE_ID" \
      --query 'Reservations[].Instances[].BlockDeviceMappings[].Ebs.VolumeId' --output text 2>/dev/null || echo "")

    for volume in $VOLUMES; do
      aws ec2 create-tags --region "$REGION" --resources "$volume" --tags \
        Key=Project,Value=databricks-wind-farm-cluster \
        Key=DeploymentID,Value="${DEPLOYMENT_ID}" \
        Key=Purpose,Value=bacalhau-storage \
        Key=ManagedBy,Value=skypilot \
        Key=Environment,Value=production 2>/dev/null || true
    done
  fi

# Run phase - start services
run: |
  set -e

  # Wait for Docker to be ready
  timeout=60
  while [ $timeout -gt 0 ]; do
    if sudo docker version >/dev/null 2>&1; then
      echo "Docker is ready"
      break
    fi
    echo "Waiting for Docker... ($timeout seconds remaining)"
    sleep 2
    timeout=$((timeout-2))
  done

  if [ $timeout -le 0 ]; then
    echo "Docker failed to start within timeout"
    exit 1
  fi

  # Generate Bacalhau configuration from orchestrator credentials
  echo "Generating Bacalhau configuration..."
  cd /opt/scripts
  sudo python3 generate_bacalhau_config.py

  # Generate node identity for sensor
  echo "Generating node identity..."
  cd /opt/sensor
  INSTANCE_ID=$(curl -s http://169.254.169.254/latest/meta-data/instance-id 2>/dev/null || echo "unknown-$(date +%s)")
  sudo INSTANCE_ID=$INSTANCE_ID python3 /opt/scripts/generate_node_identity.py

  # Start Bacalhau compute node
  echo "Starting Bacalhau compute node..."
  cd /opt/compose
  sudo docker compose -f bacalhau-compose.yml up -d

  # Wait for Bacalhau to initialize
  sleep 10

  # Start sensor simulator
  echo "Starting sensor simulator..."
  sudo docker compose -f sensor-compose.yml up -d

  # Final health check
  sleep 20
  echo "=== Deployment Status ==="
  sudo docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"

  echo "=== Bacalhau Node Status ==="
  sudo docker logs bacalhau-compute --tail 10 2>/dev/null || echo "Bacalhau logs not available yet"

  echo "=== Sensor Status ==="
  sudo docker logs sensor-simulator --tail 10 2>/dev/null || echo "Sensor logs not available yet"

  echo "=== Network Status ==="
  netstat -tlnp | grep -E ":(1234|4222|22)\s" || echo "No services found on expected ports"

  # Run health check script
  /opt/scripts/health_check.sh

  echo "Deployment complete! Node is ready for Bacalhau workloads."

  # Keep the task alive by tailing logs and monitoring services
  echo "=== Keeping cluster alive - monitoring services ==="
  while true; do
    echo "$(date): Cluster is running - Bacalhau and sensor services active"

    # Check if Docker containers are still running
    if ! sudo docker ps --filter "status=running" | grep -q bacalhau; then
      echo "WARNING: Bacalhau container not running, restarting..."
      cd /opt/compose && sudo docker compose -f bacalhau-compose.yml up -d
    fi

    if ! sudo docker ps --filter "status=running" | grep -q sensor; then
      echo "WARNING: Sensor container not running, restarting..."
      cd /opt/compose && sudo docker compose -f sensor-compose.yml up -d
    fi

    # Sleep for 5 minutes between checks
    sleep 300
  done
